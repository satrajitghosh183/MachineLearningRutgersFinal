{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Imports and Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.api import SimpleExpSmoothing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.fft import fft\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is organized into two separate folders: stocks and etf. Additionally, there is a metadata file, symbols_valid_meta.csv, which contains crucial information such as stock symbols, security names, market categories, and whether a symbol represents an ETF.\n",
    "\n",
    "The dataset is structured as follows:\n",
    "\n",
    "Metadata file (symbols_valid_meta.csv):\n",
    "Columns include Symbol, Security Name, ETF, and Market Category. This file provides descriptive context about each symbol, enabling me to differentiate between stocks and ETFs and link symbols to their respective markets.\n",
    "\n",
    "Stocks and ETF folders:\n",
    "These folders contain time-series CSV files for individual stock/ETF symbols, including columns such as Date, Open, High, Low, Close, Volume, and Adjusted Close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "META_PATH = r\"D:\\Master Things\\Fall Sem Classes\\Intro to Machine Learning\\Homework\\Project Submission\\Project Notebooks\\symbols_valid_meta.csv\"\n",
    "STOCKS_PATH = r\"D:\\Master Things\\Fall Sem Classes\\Intro to Machine Learning\\Homework\\Project Submission\\Project Notebooks\\stocks\"\n",
    "ETFS_PATH = r\"D:\\Master Things\\Fall Sem Classes\\Intro to Machine Learning\\Homework\\Project Submission\\Project Notebooks\\etfs\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyze the data structure of the dataset, I begin by importing a single sample file from the collection of multiple files in the dataset. This approach allows me to examine its structure and content in detail. Using functions like `.head()`, `.info()`, and `.describe()`, I gain a preliminary understanding of the dataset, including its columns, data types, summary statistics, and any potential data quality issues such as missing values or irregularities. This exploration helps establish a baseline for subsequent preprocessing and analysis steps.I have then proceeded to use the entire dataset to create a pdf for the analyis of all stocks and etfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyze the data structure of the dataset, I begin by importing a single file, `AAPL.csv`, from the collection of multiple files in the dataset. Focusing on this specific file allows me to examine its structure and content in detail before scaling up to the entire dataset. Using functions like `.head()`, `.info()`, and `.describe()`, I gain a preliminary understanding of the data, including its columns, data types, summary statistics, and any potential data quality issues such as missing values or irregularities. This focused exploration of the `AAPL.csv` file serves as a representative example, helping establish a baseline for subsequent preprocessing and analysis steps. Once the methodology is refined using this single file, it can be extended to analyze all files in the dataset comprehensively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = \"D:/Master Things/Fall Sem Classes/Intro to Machine Learning/Homework/Project Submission/Project Notebooks/stocks/AAPL.csv\"\n",
    "df = pd.read_csv(file_path, parse_dates=['Date'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Plot missing values as a heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
    "plt.title(\"Missing Values Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms for numerical columns\n",
    "df.hist(figsize=(12, 10), bins=30)\n",
    "plt.suptitle(\"Distribution of Numerical Features\", fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line plots for key features\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df['Date'], df['Close'], label='Close Price')\n",
    "plt.plot(df['Date'], df['Volume'] / 1e6, label='Volume (in millions)')\n",
    "plt.legend()\n",
    "plt.title(\"Time-Series Trends of Close Price and Volume\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "correlation = df[['Open', 'Close', 'High', 'Low', 'Volume']].corr()\n",
    "\n",
    "# Heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot to show pairwise relationships\n",
    "sns.pairplot(df[['Open', 'Close', 'High', 'Low', 'Volume']])\n",
    "plt.suptitle(\"Pairwise Relationships\", y=1.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "### Redfifing the path of each folder including the output folder to store the data \n",
    "stocks_path = r\"D:\\Master Things\\Fall Sem Classes\\Intro to Machine Learning\\Homework\\Project Submission\\Project Notebooks\\stocks\"\n",
    "etfs_path = r\"D:\\Master Things\\Fall Sem Classes\\Intro to Machine Learning\\Homework\\Project Submission\\Project Notebooks\\etfs\"\n",
    "output_folder = r\"D:\\Master Things\\Fall Sem Classes\\Intro to Machine Learning\\Homework\\Project Submission\\Processed_Data\"\n",
    "output_pdf_path = os.path.join(output_folder, \"Stock_ETF_Summary.pdf\")\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Function to process a single file and return DataFrame and summary\n",
    "def process_file(file_path):\n",
    "    df = pd.read_csv(file_path, parse_dates=['Date'])\n",
    "    summary = df.describe().transpose()\n",
    "    return df, summary\n",
    "\n",
    "# Function to iterate through files in a folder and generate PDF content\n",
    "def iterate_folder(folder_path, label, pdf_writer):\n",
    "    all_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "    \n",
    "    for idx, file_path in enumerate(all_files):\n",
    "        print(f\"Processing file: {os.path.basename(file_path)}\")\n",
    "        df, summary = process_file(file_path)\n",
    "\n",
    "        # Missing Value Analysis\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        missing_values = df.isnull().sum()\n",
    "        missing_values_percentage = (missing_values / len(df)) * 100\n",
    "        ax.bar(missing_values.index, missing_values_percentage)\n",
    "        ax.set_title(f\"Missing Value Analysis: {os.path.basename(file_path)}\", fontsize=14)\n",
    "        ax.set_ylabel(\"Percentage of Missing Values\")\n",
    "        ax.set_xlabel(\"Columns\")\n",
    "        pdf_writer.savefig(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "        # Summary Statistics Table\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        ax.axis('off')\n",
    "        ax.table(cellText=summary.values, colLabels=summary.columns, rowLabels=summary.index, loc='center')\n",
    "        ax.set_title(f\"Summary Statistics: {os.path.basename(file_path)}\", fontsize=14)\n",
    "        pdf_writer.savefig(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "        # Histograms for Each Feature\n",
    "        fig, ax = plt.subplots(len(df.select_dtypes(include=['float64', 'int64']).columns), 1, figsize=(12, 18))\n",
    "        num_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "        for i, col in enumerate(num_columns):\n",
    "            sns.histplot(df[col], bins=30, kde=True, ax=ax[i])\n",
    "            ax[i].set_title(f\"Histogram: {col}\")\n",
    "        plt.tight_layout()\n",
    "        pdf_writer.savefig(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "        # Time-Series Trends Analysis\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        ax.plot(df['Date'], df['Close'], label='Close Price')\n",
    "        ax.plot(df['Date'], df['High'], label='High Price')\n",
    "        ax.plot(df['Date'], df['Volume'] / 1e6, label='Volume (in millions)')\n",
    "        ax.set_title(f\"Time-Series Trends: {os.path.basename(file_path)}\", fontsize=14)\n",
    "        ax.set_xlabel(\"Date\")\n",
    "        ax.set_ylabel(\"Value\")\n",
    "        ax.legend()\n",
    "        pdf_writer.savefig(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "        # Correlation Analysis\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        correlation = df.select_dtypes(include=['float64', 'int64']).corr()\n",
    "        sns.heatmap(correlation, annot=True, cmap='coolwarm', fmt=\".2f\", ax=ax)\n",
    "        ax.set_title(f\"Correlation Heatmap: {os.path.basename(file_path)}\", fontsize=14)\n",
    "        pdf_writer.savefig(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "        # Pairwise Relationships\n",
    "        fig = sns.pairplot(df.select_dtypes(include=['float64', 'int64']), diag_kind='kde', corner=True)\n",
    "        fig.fig.suptitle(f\"Pairwise Relationships: {os.path.basename(file_path)}\", y=1.02, fontsize=14)\n",
    "        pdf_writer.savefig(fig.fig)\n",
    "        plt.close(fig.fig)\n",
    "\n",
    "# Create the output PDF\n",
    "with PdfPages(output_pdf_path) as pdf_writer:\n",
    "    # Process stocks folder\n",
    "    iterate_folder(stocks_path, \"Stocks\", pdf_writer)\n",
    "\n",
    "    # Process etfs folder\n",
    "    iterate_folder(etfs_path, \"ETFs\", pdf_writer)\n",
    "\n",
    "print(f\"Processing complete. Summary PDF saved at: {output_pdf_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2a: Data Loading and Preprocessing\n",
    "\n",
    "def load_metadata(meta_path):\n",
    "    \"\"\"Load and process symbol metadata\"\"\"\n",
    "    meta_df = pd.read_csv(meta_path)\n",
    "    symbol_info = meta_df.set_index('Symbol')[['Security Name', 'ETF', 'Market Category']].to_dict('index')\n",
    "    return symbol_info\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_rsi(prices, period=14):\n",
    "    \"\"\"Calculate Relative Strength Index\"\"\"\n",
    "    delta = prices.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def calculate_macd(prices, fast=12, slow=26, signal=9):\n",
    "    \"\"\"Calculate MACD (Moving Average Convergence Divergence)\"\"\"\n",
    "    exp1 = prices.ewm(span=fast, adjust=False).mean()\n",
    "    exp2 = prices.ewm(span=slow, adjust=False).mean()\n",
    "    macd = exp1 - exp2\n",
    "    signal_line = macd.ewm(span=signal, adjust=False).mean()\n",
    "    return macd - signal_line\n",
    "\n",
    "def create_features(df):\n",
    "    \"\"\"Create technical and statistical features\"\"\"\n",
    "    df['Returns'] = df['Close'].pct_change()\n",
    "    df['Log_Returns'] = np.log1p(df['Returns'])\n",
    "    df['Price_Range'] = df['High'] - df['Low']\n",
    "    \n",
    "    # Moving averages and technical indicators\n",
    "    for window in [5, 10, 20, 50]:\n",
    "        df[f'SMA_{window}'] = df['Close'].rolling(window=window).mean()\n",
    "        df[f'STD_{window}'] = df['Close'].rolling(window=window).std()\n",
    "    \n",
    "    df['RSI'] = calculate_rsi(df['Close'])\n",
    "    df['MACD'] = calculate_macd(df['Close'])\n",
    "    \n",
    "    df['Volume_MA5'] = df['Volume'].rolling(window=5).mean()\n",
    "    df['Volume_MA20'] = df['Volume'].rolling(window=20).mean()\n",
    "    df['Volume_Ratio'] = df['Volume'] / df['Volume_MA20']\n",
    "    \n",
    "    df['Next_Day_Return'] = df['Returns'].shift(-1)\n",
    "    df['Trend'] = np.where(df['Next_Day_Return'] > 0, 1, 0)\n",
    "    \n",
    "    return df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2b: Forecasting and Trend Classification\n",
    "\n",
    "def basic_forecasting(df, forecast_days=5):\n",
    "    \"\"\"Implement multiple basic forecasting methods\"\"\"\n",
    "    train_size = int(len(df) * 0.8)\n",
    "    train_data = df['Close'][:train_size]\n",
    "    test_data = df['Close'][train_size:]\n",
    "    \n",
    "    forecasts = {}\n",
    "    \n",
    "    # Forecasting Methods\n",
    "    sma = df['Close'].rolling(window=20).mean()\n",
    "    forecasts['SMA'] = sma\n",
    "    \n",
    "    ema = df['Close'].ewm(span=20, adjust=False).mean()\n",
    "    forecasts['EMA'] = ema\n",
    "    \n",
    "    ses_model = SimpleExpSmoothing(train_data).fit()\n",
    "    ses_forecast = ses_model.forecast(len(test_data))\n",
    "    forecasts['SES'] = pd.concat([train_data, ses_forecast])\n",
    "    \n",
    "    des_model = ExponentialSmoothing(train_data, trend='add').fit()\n",
    "    des_forecast = des_model.forecast(len(test_data))\n",
    "    forecasts['DES'] = pd.concat([train_data, des_forecast])\n",
    "    \n",
    "    # Calculate Errors\n",
    "    errors = {}\n",
    "    for method, forecast in forecasts.items():\n",
    "        error = np.sqrt(mean_squared_error(test_data, forecast[train_size:train_size+len(test_data)]))\n",
    "        errors[method] = error\n",
    "    \n",
    "    return forecasts, errors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trend_classification(df):\n",
    "    \"\"\"Implement Random Forest for trend classification\"\"\"\n",
    "    feature_cols = ['Returns', 'Price_Range', 'SMA_5', 'SMA_20', 'STD_20', 'RSI', 'MACD', 'Volume_Ratio']\n",
    "    X = df[feature_cols]\n",
    "    y = df['Trend']\n",
    "    \n",
    "    train_size = int(len(df) * 0.8)\n",
    "    X_train = X[:train_size]\n",
    "    X_test = X[train_size:]\n",
    "    y_train = y[:train_size]\n",
    "    y_test = y[train_size:]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    rf_model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "    rf_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    y_pred = rf_model.predict(X_test_scaled)\n",
    "    importance_df = pd.DataFrame({'feature': feature_cols, 'importance': rf_model.feature_importances_}).sort_values('importance', ascending=False)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'report': classification_report(y_test, y_pred),\n",
    "        'importance': importance_df,\n",
    "        'predictions': y_pred,\n",
    "        'true_values': y_test\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2c: Visualization Functions\n",
    "\n",
    "def plot_price_and_forecasts(df, forecasts, symbol):\n",
    "    \"\"\"Plot actual prices with different forecasting methods\"\"\"\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=df.index, y=df['Close'], name='Actual Price', line=dict(color='black')))\n",
    "    \n",
    "    colors = ['blue', 'red', 'green', 'purple']\n",
    "    for (method, forecast), color in zip(forecasts.items(), colors):\n",
    "        fig.add_trace(go.Scatter(x=df.index, y=forecast, name=f'{method} Forecast', line=dict(color=color, dash='dash')))\n",
    "    \n",
    "    fig.update_layout(title=f'Price Forecasts - {symbol}', xaxis_title='Date', yaxis_title='Price', height=600, showlegend=True)\n",
    "    fig.show()\n",
    "\n",
    "def plot_trend_analysis(results, symbol):\n",
    "    \"\"\"Plot trend classification results\"\"\"\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Feature Importance', 'Prediction Distribution', 'Confusion Matrix', 'Prediction Timeline'),\n",
    "        specs=[[{\"type\": \"bar\"}, {\"type\": \"domain\"}], [{\"type\": \"heatmap\"}, {\"type\": \"scatter\"}]]\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(go.Bar(x=results['importance']['importance'], y=results['importance']['feature'], orientation='h'), row=1, col=1)\n",
    "    \n",
    "    pred_dist = pd.Series(results['predictions']).value_counts()\n",
    "    fig.add_trace(go.Pie(labels=['Downtrend', 'Uptrend'], values=[pred_dist.get(0, 0), pred_dist.get(1, 0)]), row=1, col=2)\n",
    "    \n",
    "    cm = pd.crosstab(results['true_values'], results['predictions'])\n",
    "    fig.add_trace(go.Heatmap(z=cm.values, x=['Pred Down', 'Pred Up'], y=['True Down', 'True Up']), row=2, col=1)\n",
    "    \n",
    "    fig.add_trace(go.Scatter(x=np.arange(len(results['predictions'])), y=results['predictions'], mode='markers', marker=dict(color=results['predictions'], colorscale='RdYlGn')), row=2, col=2)\n",
    "    \n",
    "    fig.update_layout(height=800, title_text=f\"Trend Analysis - {symbol}\", showlegend=False)\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3: Main Analysis Function\n",
    "\n",
    "def analyze_stock(symbol, is_etf=False):\n",
    "    \"\"\"Complete analysis pipeline for a single stock\"\"\"\n",
    "    symbol_info = load_metadata(META_PATH)\n",
    "    \n",
    "    base_path = ETFS_PATH if is_etf else STOCKS_PATH\n",
    "    file_path = os.path.join(base_path, f\"{symbol}.csv\")\n",
    "    \n",
    "    df = pd.read_csv(file_path)\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df = df.set_index('Date')\n",
    "    \n",
    "    if symbol in symbol_info:\n",
    "        df['Security_Name'] = symbol_info[symbol]['Security Name']\n",
    "        df['Is_ETF'] = symbol_info[symbol]['ETF']\n",
    "    else:\n",
    "        df['Security_Name'] = symbol\n",
    "        df['Is_ETF'] = 'Y' if is_etf else 'N'\n",
    "    \n",
    "    df = create_features(df)\n",
    "    forecasts, errors = basic_forecasting(df)\n",
    "    trend_results = trend_classification(df)\n",
    "    \n",
    "    plot_price_and_forecasts(df, forecasts, symbol)\n",
    "    plot_trend_analysis(trend_results, symbol)\n",
    "    \n",
    "    print(f\"\\nResults for {symbol}\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"\\nForecasting RMSE:\")\n",
    "    for method, error in errors.items():\n",
    "        print(f\"{method}: {error:.2f}\")\n",
    "    \n",
    "    print(\"\\nTrend Classification Report:\")\n",
    "    print(trend_results['report'])\n",
    "    \n",
    "    return {'symbol': symbol, 'forecast_errors': errors, 'trend_accuracy': trend_results['accuracy']}\n",
    "\n",
    "# Example Usage\n",
    "# Analyze a stock (e.g., 'AAPL')\n",
    "results = analyze_stock('AAPL', is_etf=False)\n",
    "\n",
    "# Analyze an ETF (e.g., 'SPY')\n",
    "# results = analyze_stock('SPY', is_etf=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load metadata with symbols (assuming it has a 'symbol' column for stock symbols)\n",
    "metadata_path = r\"D:\\Master Things\\Fall Sem Classes\\Intro to Machine Learning\\Homework\\Project Submission\\Project Notebooks\\symbols_valid_meta.csv\"\n",
    "metadata_df = pd.read_csv(metadata_path)\n",
    "random_symbols = metadata_df['Symbol'].sample(n=5, random_state=42)  # Setting random_state for reproducibility\n",
    "\n",
    "all_results = []\n",
    "\n",
    "# Loop through each symbol in the random sample and run the analysis\n",
    "for symbol in random_symbols:\n",
    "    try:\n",
    "        # Analyze each stock (set is_etf based on your criteria)\n",
    "        result = analyze_stock(symbol, is_etf=False)\n",
    "        all_results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing {symbol}: {e}\")\n",
    "\n",
    "# Convert results into a DataFrame for easier access\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Display or save the results DataFrame\n",
    "print(results_df)\n",
    "# Optionally, save to CSV\n",
    "results_df.to_csv(\"all_stock_analysis_results.csv\", index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
